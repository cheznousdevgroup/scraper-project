import os
from dotenv import load_dotenv
from proxy_manager import ProxyManager
from locanto_scraper_final import LocantoScraperFinal
import json
import time
from datetime import datetime

def save_checkpoint(data: dict, filename: str):
    """Sauvegarde progressive"""
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def main():
    load_dotenv()
    
    print("="*70)
    print("üï∑Ô∏è  LOCANTO SCRAPER - PRODUCTION - SCRAPING COMPLET")
    print("="*70)
    
    # Init
    proxy_manager = ProxyManager()
    if not proxy_manager.test_proxy():
        return
    
    scraper = LocantoScraperFinal(proxy_manager)
    
    # Config
    site_url = os.getenv('SITE_URL', 'https://abidjan.locanto.ci/')
    max_categories = int(os.getenv('MAX_CATEGORIES', 999))  # Toutes les cat√©gories
    max_listings = int(os.getenv('MAX_LISTINGS', 50))  # 50 annonces par cat√©gorie
    max_pages = int(os.getenv('MAX_PAGES', 5))  # 5 pages par cat√©gorie
    
    print(f"\n‚öôÔ∏è  CONFIGURATION")
    print(f"   Site: {site_url}")
    print(f"   Cat√©gories max: {max_categories if max_categories < 999 else 'TOUTES'}")
    print(f"   Annonces/cat√©gorie: {max_listings}")
    print(f"   Pages/cat√©gorie: {max_pages}")
    
    # Timestamp
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    country = site_url.split('//')[1].split('.')[0].replace('www', 'main')
    
    output_dir = '/app/data/full_scrapes'
    os.makedirs(output_dir, exist_ok=True)
    
    filename = f"{output_dir}/{country}_{timestamp}.json"
    
    print(f"   Sortie: {filename}")
    print(f"\n{'='*70}")
    
    start_time = time.time()
    
    # D√©but du scraping
    result = {
        'site_url': site_url,
        'scrape_date': datetime.now().isoformat(),
        'config': {
            'max_categories': max_categories,
            'max_listings_per_category': max_listings,
            'max_pages_per_category': max_pages
        },
        'categories': [],
        'stats': {
            'total_listings': 0,
            'total_categories': 0,
            'errors': 0
        }
    }
    
    print(f"\nüåç SCRAPING: {site_url}\n")
    print("="*70)
    
    # Extraire cat√©gories
    categories = scraper.get_categories(site_url)
    
    if not categories:
        print("‚ùå Aucune cat√©gorie trouv√©e")
        return
    
    total_cats = min(len(categories), max_categories)
    
    # Scraper chaque cat√©gorie
    for i, category in enumerate(categories[:max_categories], 1):
        try:
            print(f"\n{'='*70}")
            print(f"üìÅ [{i}/{total_cats}] {category['name']}")
            print(f"   {category['url']}")
            print("="*70)
            
            # Extraire URLs annonces
            listing_urls = scraper.get_listings_from_category(category['url'], max_pages=max_pages)
            print(f"\n   üìã {len(listing_urls)} URLs collect√©es")
            
            if not listing_urls:
                print(f"   ‚ö†Ô∏è Aucune annonce dans cette cat√©gorie")
                continue
            
            # Extraire d√©tails
            listings = []
            errors = 0
            
            for j, url in enumerate(listing_urls[:max_listings], 1):
                print(f"   [{j}/{min(max_listings, len(listing_urls))}]", end=' ')
                
                try:
                    details = scraper.get_listing_details(url)
                    if details:
                        listings.append(details)
                except Exception as e:
                    errors += 1
                    print(f"      ‚ùå Erreur: {str(e)[:40]}")
            
            cat_data = {
                'name': category['name'],
                'url': category['url'],
                'listings_found': len(listing_urls),
                'listings_scraped': len(listings),
                'listings': listings,
                'errors': errors
            }
            
            result['categories'].append(cat_data)
            result['stats']['total_listings'] += len(listings)
            result['stats']['total_categories'] += 1
            result['stats']['errors'] += errors
            
            # Sauvegarde progressive tous les 5 cat√©gories
            if i % 5 == 0:
                save_checkpoint(result, filename)
                print(f"\n   üíæ Checkpoint sauvegard√© ({i} cat√©gories)")
            
            print(f"\n   ‚úÖ {len(listings)} annonces extraites ({errors} erreurs)")
            
            # Pause entre cat√©gories
            if i < total_cats:
                time.sleep(3)
        
        except KeyboardInterrupt:
            print(f"\n\n‚ö†Ô∏è Interruption utilisateur")
            break
        except Exception as e:
            print(f"\n   ‚ùå Erreur cat√©gorie: {e}")
            result['stats']['errors'] += 1
            continue
    
    # Sauvegarde finale
    save_checkpoint(result, filename)
    
    # Stats finales
    duration = time.time() - start_time
    
    print(f"\n{'='*70}")
    print(f"‚úÖ SCRAPING TERMIN√â")
    print("="*70)
    print(f"\nüìä STATISTIQUES:")
    print(f"   ‚Ä¢ Dur√©e totale: {duration/60:.1f} minutes")
    print(f"   ‚Ä¢ Cat√©gories scrap√©es: {result['stats']['total_categories']}")
    print(f"   ‚Ä¢ Annonces extraites: {result['stats']['total_listings']}")
    print(f"   ‚Ä¢ Erreurs: {result['stats']['errors']}")
    print(f"   ‚Ä¢ Vitesse: {result['stats']['total_listings']/(duration/60):.1f} annonces/min")
    
    # Top cat√©gories
    print(f"\nüìà TOP 5 CAT√âGORIES:")
    top_cats = sorted(result['categories'], key=lambda x: len(x['listings']), reverse=True)[:5]
    for i, cat in enumerate(top_cats, 1):
        print(f"   {i}. {cat['name']}: {len(cat['listings'])} annonces")
    
    print(f"\nüíæ Fichier: {filename}")
    print(f"\n{'='*70}")
    
    # G√©n√©rer rapport CSV
    csv_filename = filename.replace('.json', '_summary.csv')
    with open(csv_filename, 'w', encoding='utf-8') as f:
        f.write("Cat√©gorie,URL,Annonces Trouv√©es,Annonces Extraites,Erreurs\n")
        for cat in result['categories']:
            f.write(f'"{cat["name"]}","{cat["url"]}",{cat["listings_found"]},{cat["listings_scraped"]},{cat["errors"]}\n')
    
    print(f"üìÑ Rapport CSV: {csv_filename}\n")

if __name__ == "__main__":
    main()